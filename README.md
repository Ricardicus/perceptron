# Perceptron
A multilayered perceptron artificial neural network in python!
It can to regression and binary classification today.
I might extend it further in the future to make
multiple label classification also!

# Train
One can define the number of layers in the 'hidden_layers' vector.
Also in the last hidden-output layer one can determine what 
activation and what cost function to use.
The following configurations are recommended:

<table>
  <thead>
    <tr>
      <th>Problem</th>
      <th>Hidden-output layer activation</th>
      <th>Error function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Regression</td>
      <td>'linear'</td>
      <td>'square sum'</td>   
    </tr>
    <tr>
      <td>Binary classification</td>
      <td>'sigmoid'</td>
      <td>'binary_crossentropy'</td>
    </tr>
    <tr>
      <td>Multiple classes classification</td>
      <td>'softmax'</td>
      <td>'crossentropy'</td>
    </tr>
  </tbody>
</table>
  
The module 'examples.py' contains traning examples of each of these problems. Note that both the X-values and the Y-values are to be lists of numpy column vectors. 

# Examples 

A solution to the binary classification problem, not fully over-trained, generated by this program (decision boundary drawn):

![Binary classification](examples/gauss_classifier_H100-100-100-10-10.png?raw=true "Binary Classification")

A solution to a regression problem generated by this program:

![Function regression](examples/sin_regression_H10-10-10.png?raw=true "Binary Classification")

# Note
Setting the correct value of parameters such as learning rate is depending
on the problem and the data it is training on. Parameters should be tuned 
according to the way it performs. For example if the loss function is rising, 
a too large  learning rate is typically in use. Lower it in that case. 
